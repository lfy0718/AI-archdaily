import os
import json
import time
import requests
from bs4 import BeautifulSoup

# ArchDaily æœç´¢ç»“æœ URL
ARCHDAILY_SEARCH_URL = "https://www.archdaily.com/search/projects?adkq=building&page="

# è¯·æ±‚å¤´ï¼Œé¿å…åçˆ¬
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# å›¾ç‰‡ä¿å­˜ç›®å½•
IMG_FOLDER = "archdaily_images"
if not os.path.exists(IMG_FOLDER):
    os.makedirs(IMG_FOLDER)

# é¡¹ç›®æ•°æ®å­˜å‚¨æ–‡ä»¶
DATA_FILE = "projects.json"


def get_project_links(page=1):
    """ è·å–æœç´¢ç»“æœé¡µé¢çš„é¡¹ç›®é“¾æ¥ """
    url = ARCHDAILY_SEARCH_URL + str(page)
    print(f"ğŸ” çˆ¬å–æœç´¢é¡µ: {url}")

    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print("âŒ æ— æ³•è®¿é—® ArchDailyï¼ŒçŠ¶æ€ç :", response.status_code)
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    project_elements = soup.find_all("div", class_="afd-search-list__item")

    if not project_elements:
        print("âš ï¸ æœªæ‰¾åˆ°é¡¹ç›®å…ƒç´ ï¼Œå¯èƒ½æ˜¯ç½‘é¡µç»“æ„æ”¹å˜æˆ–è¢«åçˆ¬ï¼")
        return []

    project_links = ["https://www.archdaily.com" + item.find("a")["href"] for item in project_elements if
                     item.find("a")]
    print(f"âœ… æ‰¾åˆ° {len(project_links)} ä¸ªé¡¹ç›®")

    return project_links[:5]  # ä»…è·å–å‰ 5 ä¸ª


def scrape_project_details(project_url):
    """ è·å–é¡¹ç›®è¯¦æƒ…ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€ä»‹ç»å’Œå›¾ç‰‡ """
    print(f"ğŸ“¡ çˆ¬å–é¡¹ç›®é¡µé¢: {project_url}")

    response = requests.get(project_url, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {project_url}ï¼ŒçŠ¶æ€ç : {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # è·å–é¡¹ç›®æ ‡é¢˜
    title_tag = soup.find("h1", class_="afd-title-big")
    title = title_tag.text.strip() if title_tag else "æœªçŸ¥é¡¹ç›®"

    # è·å–é¡¹ç›®ä»‹ç»
    description_tag = soup.find("div", class_="afd-paragraph-big")
    description = description_tag.text.strip() if description_tag else "æš‚æ— ä»‹ç»"

    # è·å–æ‰€æœ‰å›¾ç‰‡
    img_tags = soup.find_all("img", class_="picture--content__img")
    img_urls = [img["src"] for img in img_tags if "src" in img.attrs]

    # ä¸‹è½½å›¾ç‰‡
    saved_images = []
    for img_url in img_urls[:3]:  # ä»…ä¸‹è½½å‰ 3 å¼ å›¾ç‰‡ï¼Œé¿å…è¿‡å¤š
        img_path = download_image(img_url, title)
        if img_path:
            saved_images.append(img_path)

    return {
        "title": title,
        "description": description,
        "images": saved_images,
        "url": project_url
    }


def download_image(img_url, title):
    """ ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜ """
    print(f"ğŸŒ ä¸‹è½½å›¾ç‰‡: {img_url}")

    try:
        response = requests.get(img_url, headers=HEADERS, stream=True)
        if response.status_code == 200:
            filename = f"{IMG_FOLDER}/{title.replace(' ', '_')}.jpg"
            with open(filename, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)
            print(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {filename}")
            return filename
        else:
            print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url}")
    except Exception as e:
        print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å‡ºé”™: {e}")
    return None


def save_data(data):
    """ ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶ """
    with open(DATA_FILE, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=4)
    print(f"ğŸ’¾ æ•°æ®ä¿å­˜æˆåŠŸ: {DATA_FILE}")


def main():
    """ çˆ¬å– ArchDaily é¡¹ç›®æ•°æ® """
    print("ğŸš€ å¼€å§‹çˆ¬å– ArchDaily é¡¹ç›®æ•°æ®...")

    project_links = get_project_links()
    if not project_links:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®ï¼Œç¨‹åºé€€å‡º")
        return

    all_projects = []
    for project_url in project_links:
        project_data = scrape_project_details(project_url)
        if project_data:
            all_projects.append(project_data)

        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«è¢«å°

    # ä¿å­˜æ•°æ®
    save_data(all_projects)
    print("ğŸ‰ çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    main()
