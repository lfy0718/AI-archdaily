# -*- coding: utf-8 -*-
# @Author  : Yiheng Feng
# @Time    : 4/21/2025 3:40 PM
# @Function:
import atexit
import json
import logging
import os
import random
import threading
import time
import traceback
import warnings
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Callable, Optional, Any

import numpy as np
import requests
import streamlit as st
from tqdm import tqdm

from utils import db_utils

logging.info("Backend Reloaded ============================================================")


@st.cache_resource
def load_config():
    # config will only be loaded once during the whole lifetime
    # ä¹Ÿå¯ä»¥è®©è£…é¥°å™¨æ”¹ä¸ºst.cacheï¼Œ è¿™æ ·æ¯æ¬¡åˆ·æ–°é¡µé¢å°±ä¼šé‡æ–°åŠ è½½ï¼Œå¯ä»¥æ–¹ä¾¿è°ƒè¯•
    import config
    return config


config = load_config()  # use cached config
user_settings = config.user_settings


class WorkingContext:
    # è‡ªå®šä¹‰çš„åˆ›å»ºå·¥ä½œä»»åŠ¡çš„ä¸Šä¸‹æ–‡
    def __init__(self, ctx_name: str,
                 working_content: Callable[['WorkingContext', Any, ...], None],
                 *args,
                 total: Optional[int] = None,
                 on_complete_callback: Callable[[], None] = None,
                 singleton=True,
                 enable_ctx_scope_check=False):

        self._ctx_name = ctx_name
        self._working_content = working_content
        self.args = args
        self._on_complete_callback = on_complete_callback
        self._curr = 0
        self._total = total if total is not None else 1
        self._singleton = singleton
        self._enable_ctx_scope_check = enable_ctx_scope_check
        self._should_stop = False
        self._is_running = False
        self._success = False
        self._msg = ""

        self._success_projects = []
        self._failed_projects = []
        self._running_projects = []
        self._complete_projects = []
        self._project_start_times: dict[str: float] = {}
        self._project_sub_curr: dict[str: int] = {}
        self._project_sub_total: dict[str: int] = {}

        self._custom_data = {}

    def start_work(self) -> None:
        if self._singleton and len(g.running_context) > 0:
            self._msg = f"{self._ctx_name}.singleton set to True and Another tasks is running"
            logging.warning(self._msg)
            return
        if self._enable_ctx_scope_check and not WorkingContext.check_ctx_scope(self._ctx_name):
            self._msg = f"Enable Context Scope Check enabled, but current scope: {self._ctx_name} is not same as last scope: {g.last_context_name}"
            logging.warning(self._msg)
            self._success = False
            return
        if self._total is None or self._total <= 0:
            self._msg = f"{self._ctx_name} total is None or total <= 0"
            logging.warning(self._msg)
            self._success = False
            return
        if self._ctx_name in g.running_context:
            self._msg = f"{self._ctx_name} is already running"
            logging.warning(self._msg)
            self._success = False
            return

        g.running_context[self._ctx_name] = self
        self._is_running = True
        self._should_stop = False
        self._curr = 0
        self._success = False
        self._msg = f"{self._ctx_name} å¼€å§‹æ‰§è¡Œ"
        logging.info(self._msg)

        def _func():
            time.sleep(0.1)  # åŠ ä¸€ä¸ªå°å°çš„å»¶è¿Ÿï¼Œç­‰å¾…UIæ›´æ–°å®Œæ¯•åå†å¼€å§‹
            try:
                # ==========================================================================================
                self._working_content(self, *self.args)  # main content éœ€è¦ä¼ å…¥ctxå‚æ•°
                # ==========================================================================================
                if self._on_complete_callback:
                    logging.info(f"{self._ctx_name} æ­£åœ¨æ‰§è¡Œå®Œæˆå›è°ƒå‡½æ•°")
                    self._on_complete_callback()
                self._success = True
                self._msg = f"{self._ctx_name} å®Œæˆ"
                logging.info(self._msg)
            except Exception as e:
                self._success = False
                self._msg = f"{self._ctx_name} æ‰§è¡Œå‡ºé”™, {e}"
                logging.warning(self._msg)
                traceback.print_exc()
            finally:
                g.running_context.pop(self._ctx_name)
                self._is_running = False
                g.last_context_name = self._ctx_name
                logging.info(f"last_context_nameå·²æ›´æ–°ä¸º:{g.last_context_name}")

        threading.Thread(target=_func).start()

    def stop_work(self):
        self._should_stop = True  # æ ‡è®°ä¸ºåœæ­¢ï¼Œå¹¶ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œçº¿ç¨‹ä¸­çš„å¾ªç¯éœ€è¦åŠ å…¥å¯¹_should_stopçš„åˆ¤æ–­
        while self._is_running:
            time.sleep(0.1)

    def get_status(self):
        status = {'is_running': self._is_running, 'success': self._success,
                  'msg': self._msg, 'curr': self._curr, 'total': self._total, 'should_stop': self._should_stop}
        status.update(self.custom_data)
        return status

    # region project related status
    def get_lasting_time(self, project_id):
        if project_id in self._project_start_times:
            return time.time() - self._project_start_times[project_id]
        else:
            return -1

    def get_project_sub_curr(self, project_id):
        if project_id in self._project_sub_curr:
            return self._project_sub_curr[project_id]
        else:
            return -1

    def get_project_sub_total(self, project_id):
        if project_id in self._project_sub_total:
            return self._project_sub_total[project_id]
        else:
            return -1

    def get_project_detail_info_str(self, project_id):
        if project_id in self._running_projects:
            if project_id in self._project_sub_curr and project_id in self._project_sub_total:
                sub_curr = self.get_project_sub_curr(project_id)
                sub_total = self.get_project_sub_total(project_id)
                return f"{project_id} [{sub_curr}/{sub_total}] {int(self.get_lasting_time(project_id))}s"
            else:
                return f"{project_id} {int(self.get_lasting_time(project_id))}s"
        else:
            return f"{project_id} Complete"

    # endregion

    # region put these in working content
    def set_total(self, total):
        self._total = total

    def set_curr(self, value):
        self._curr = value

    def update(self, count=1):
        self._curr += count

    def report_msg(self, msg):
        self._msg = msg
        logging.info(msg)

    def report_project_start(self, project_id):
        """æŠ¥å‘Šé¡¹ç›®å¼€å§‹"""
        project_id = str(project_id)
        if project_id in self._running_projects:
            return
        self._running_projects.append(project_id)
        self._project_start_times[project_id] = time.time()

    def _on_project_complete(self, project_id):
        if project_id in self._running_projects:
            self._running_projects.remove(project_id)
        if project_id in self._project_start_times:
            self._project_start_times.pop(project_id)
        if project_id in self._project_sub_total:
            self._project_sub_total.pop(project_id)
        if project_id in self._project_sub_curr:
            self._project_sub_curr.pop(project_id)

    def report_project_complete(self, project_id):
        """æŠ¥å‘Šé¡¹ç›®å®Œæˆ"""
        project_id = str(project_id)
        self._complete_projects.append(project_id)
        self._on_project_complete(project_id)

    def report_project_success(self, project_id):
        """æŠ¥å‘Šé¡¹ç›®å®Œæˆå¹¶æˆåŠŸ"""
        project_id = str(project_id)
        self._success_projects.append(project_id)
        self._complete_projects.append(project_id)
        self._on_project_complete(project_id)

    def report_project_failed(self, project_id):
        """æŠ¥å‘Šé¡¹ç›®å®Œæˆå¹¶å¤±è´¥"""
        project_id = str(project_id)
        self._failed_projects.append(project_id)
        self._complete_projects.append(project_id)
        self._on_project_complete(project_id)

    def report_project_sub_total(self, project_id, total):
        project_id = str(project_id)
        self._project_sub_total[project_id] = total

    def report_project_sub_curr(self, project_id, curr):
        project_id = str(project_id)
        self._project_sub_curr[project_id] = curr

    @property
    def custom_data(self):
        return self._custom_data

    @property
    def should_stop(self):
        return self._should_stop

    # endregion

    @property
    def success_projects(self):
        return self._success_projects

    @property
    def failed_projects(self):
        return self._failed_projects

    @property
    def running_projects(self):
        return self._running_projects

    @staticmethod
    def check_ctx_scope(ctx_name):
        ctx_name = ctx_name.replace("_", "-")
        if g.last_context_name != "":
            last_scope = g.last_context_name.split("-")[0]
            curr_scope = ctx_name.split("-")[0]
            if last_scope != curr_scope:
                return False
        return True


class GlobalAppState:
    def __init__(self):
        from utils.html_utils import ArchdailyFlags, GoooodFlags
        self.running_context: dict[str: 'WorkingContext'] = {}
        self.last_context_name = ""
        self.project_id_queue = []

        self.flag_states = {
            "archdaily": {flag.name: False for flag in ArchdailyFlags if flag != ArchdailyFlags.NONE},
            "gooood": {flag.name: False for flag in GoooodFlags if flag != GoooodFlags.NONE}
        }

        self.flag_name_to_flag = {
            "archdaily": {flag.name: flag for flag in ArchdailyFlags},
            "gooood": {flag.name: flag for flag in GoooodFlags}
        }

        # db
        self.mongo_client = None

        atexit.register(self.close_mongo_client)

    def close_mongo_client(self):
        if self.mongo_client is not None:
            logging.info("mongodb clientè¿æ¥å·²å…³é—­")
            self.mongo_client.close()


@st.cache_resource
def create_global_app_state():
    return GlobalAppState()


g = create_global_app_state()  # this g is shared by all users and only be loaded once


# region UI Templates
def template_flags(flag_type):
    if flag_type not in g.flag_states:
        st.warning(f"flag_states[{flag_type}] not found")
        return

    def on_change(_flag_name: str):
        ss_key_value = st.session_state[f'key_{flag_type}_{_flag_name}']
        st.session_state[f"{flag_type}_{_flag_name}"] = ss_key_value
        g.flag_states[flag_type][_flag_name] = ss_key_value
        logging.info(f"{flag_type}.{_flag_name} set to {ss_key_value}")

    def make_on_change(_flag_name: str):
        # åˆ›å»ºé—­åŒ…å‡½æ•°
        return lambda: on_change(_flag_name)

    for flag_name in g.flag_states[flag_type]:
        ss_name = f"{flag_type}_{flag_name}"
        if ss_name not in st.session_state:
            st.session_state[ss_name] = g.flag_states[flag_type][flag_name]
        st.checkbox(flag_name, value=st.session_state[ss_name], key=f'key_{flag_type}_{flag_name}', on_change=make_on_change(flag_name))


def template_project_id_queue_info_box(name: str, ctx_name: str):
    if len(g.project_id_queue) == 0:
        st.info(f"æ²¡æœ‰{name}")
    elif not WorkingContext.check_ctx_scope(ctx_name):
        # st.warning(f"å½“å‰project_id_queueçš„é¡¹ç›®ä¸ºå…¶ä»–{g.last_context_name}, ä¸é€‚ç”¨äºå½“å‰{ctx_name}")
        pass
    else:
        st.info(f"{name}æ•°é‡: {len(g.project_id_queue)}")


def template_start_work_with_progress(label, ctx_name, working_content: Callable[['WorkingContext', Any, ...], None],
                                      *args,
                                      ctx_singleton=True, ctx_enable_ctx_scope_check=False,
                                      st_show_detail_number=False, st_show_detail_project_id=False,
                                      st_button_type='primary', st_button_icon=None) -> dict:
    disabled = False
    suffix = ""
    if ctx_enable_ctx_scope_check:
        # pre-check
        if not WorkingContext.check_ctx_scope(ctx_name):
            disabled |= True
            suffix += " ctx_scope unmatch"
    if ctx_name in g.running_context:
        disabled |= True
        suffix += " running"

    if st.button(label + suffix, use_container_width=True, type=st_button_type, key=f"run_{ctx_name}",
                 icon=st_button_icon, disabled=disabled):
        logging.info(f"{ctx_name}æŒ‰é’®è¢«ç‚¹å‡»")
        ctx = WorkingContext(ctx_name, working_content, *args, singleton=ctx_singleton,
                             enable_ctx_scope_check=ctx_enable_ctx_scope_check)
        ctx.start_work()

    if ctx_name not in g.running_context:
        # st.text(f"{ctx_name}ä¸åœ¨è¿è¡Œ")
        return {}

    # ä¸‹é¢éƒ½æ˜¯ctxæ­£åœ¨è¿è¡Œçš„ä»£ç 
    ctx: WorkingContext = g.running_context[ctx_name]
    # region placeholders
    stop_placeholder = st.empty()
    progress_placeholder = st.empty()
    running_projects_placeholder, success_projects_placeholder, failed_projects_placeholder = None, None, None
    running_projects_detail_placeholder, success_projects_detail_placeholder, failed_projects_detail_placeholder = None, None, None

    if st_show_detail_number:
        col1, col2, col3 = st.columns(3)
        running_projects_placeholder = col1.empty()
        success_projects_placeholder = col2.empty()
        failed_projects_placeholder = col3.empty()

        if st_show_detail_project_id:
            expander = col1.expander("View Running Projects", icon="ğŸ”¥")
            running_projects_detail_placeholder = expander.empty()
            expander = col2.expander("View Success Projects", icon="âœ…")
            success_projects_detail_placeholder = expander.empty()
            expander = col3.expander("View Failed Projects", icon="âŒ")
            failed_projects_detail_placeholder = expander.empty()
    # endregion
    if stop_placeholder.button("Stop", use_container_width=True, key="stop_" + ctx_name, disabled=ctx is None):
        logging.warning(f"{ctx_name}æ­£åœ¨åœæ­¢")
        ctx.stop_work()
    while True:
        time.sleep(0.5)
        status = ctx.get_status()
        is_running = status['is_running']
        should_stop = status['should_stop']
        # region these components update every 0.5 seconds
        progress_placeholder.progress(status['curr'] / status['total'], text=f"{status['curr']}/{status['total']}")
        if st_show_detail_number:
            running_projects_placeholder.text(f"è¿è¡Œä¸­: {len(ctx.running_projects)}")
            success_projects_placeholder.text(f"æˆåŠŸ: {len(ctx.success_projects)}")
            failed_projects_placeholder.text(f"å¤±è´¥: {len(ctx.failed_projects)}")
            if st_show_detail_project_id:
                running_projects_detail_placeholder.text(
                    '\n'.join([ctx.get_project_detail_info_str(project_id) for project_id in ctx.running_projects]))
                success_projects_detail_placeholder.text('\n'.join(ctx.success_projects[-10:][::-1]))
                failed_projects_detail_placeholder.text('\n'.join(ctx.failed_projects[-10:][::-1]))
        if is_running and should_stop:
            stop_placeholder.text("ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
        # endregion
        if not is_running:
            break

    # è¿è¡Œå®Œæˆ ==================================
    # clear placeholders
    stop_placeholder.empty()
    progress_placeholder.empty()
    if status['success']:
        st.success(f"{status['msg']}")
    else:
        st.warning(f"{status['msg']}")
    return status


def template_mongodb_connection_region(db_name, db_name_setter: Callable[[str], None]) -> bool:
    if g.mongo_client is None:
        db_host = st.text_input("MongoDB Host", user_settings.mongodb_host)
        col1, col2 = st.columns(2)
        if col1.button("ä¿å­˜", use_container_width=True):
            user_settings.mongodb_host = db_host
            config.save_user_settings(user_settings)
        if col2.button("ä¿å­˜å¹¶è¿æ¥", use_container_width=True, type="primary"):
            user_settings.mongodb_host = db_host
            config.save_user_settings(user_settings)
            success, g.mongo_client = db_utils.get_mongo_client(db_host)
            if not success:
                st.warning("è¿æ¥å¤±è´¥")
            else:
                st.rerun()
        return False
    assert g.mongo_client is not None
    db_names = g.mongo_client.list_database_names()
    if db_name not in db_names:
        st.warning(f"æ— æ³•è¿æ¥åˆ°æ•°æ®åº“({db_name})")
        db_name = st.text_input("DB Name", db_name)
        if st.button("ä¿å­˜å¹¶åˆ·æ–°"):
            db_name_setter(db_name)
            config.save_user_settings(user_settings)
            logging.info(f"å·²åˆ‡æ¢è‡³æ•°æ®åº“({db_name})")
            st.rerun()
    else:
        col1, col2 = st.columns([8, 2])
        col1.success(f"ğŸŒ¿å·²æˆåŠŸè¿æ¥è‡³æ•°æ®åº“ {db_name}")
        if col2.button("æ–­å¼€è¿æ¥", use_container_width=True):
            g.mongo_client.close()
            g.mongo_client = None
            st.rerun()


# endregion


# region Archdaily Functions
def archdaily__scan_projects_with_no_content_html(ctx: WorkingContext, *args):
    _ = args
    _all_projects = os.listdir(user_settings.archdaily_projects_dir)
    ctx.set_total(len(_all_projects))
    g.project_id_queue = []
    for project_id in _all_projects:
        ctx.report_project_start(project_id)
        if ctx.should_stop:
            break
        ctx.update(1)
        folder_path = os.path.join(user_settings.archdaily_projects_dir, project_id)
        if os.path.isdir(folder_path):
            html_file_path = os.path.join(folder_path, f'content.html')  # æ‰«ææ˜¯å¦æœ‰content.html
            if not os.path.exists(html_file_path):
                g.project_id_queue.append(project_id)
        ctx.report_project_success(project_id)


def archdaily__scan_valid_project_id_in_range(ctx: WorkingContext, start_id: int, end_id: int):
    ctx.set_total(4)

    # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½invalid_project_ids
    if os.path.exists(user_settings.archdaily_invalid_projects_ids_path):
        with open(user_settings.archdaily_invalid_projects_ids_path, 'r', encoding='utf-8') as f:
            invalid_project_ids = set(json.load(f))
    else:
        invalid_project_ids = set()
    ctx.set_curr(1)
    id_range = list(range(start_id, end_id + 1)) if start_id <= end_id else list(range(end_id, start_id + 1))
    if start_id > end_id:
        id_range.reverse()
    project_id_queue_full: list[str] = [str(project_id) for project_id in id_range]
    ctx.set_curr(2)
    # æ‰£é™¤all_projectså·²ç»å­˜åœ¨çš„é¡¹ç›®
    all_projects_set = set(os.listdir(user_settings.archdaily_projects_dir))
    project_id_queue = [project_id for project_id in project_id_queue_full if
                        project_id not in all_projects_set]
    ctx.set_curr(3)
    # æ‰£é™¤invalid_project_ids
    project_id_queue = [project_id for project_id in project_id_queue if
                        project_id not in invalid_project_ids]
    ctx.set_curr(4)
    g.project_id_queue = project_id_queue


def archdaily__scan_projects_folder_for_parsing_content(ctx: WorkingContext, skip_exist=False, *args):
    _ = args
    _all_projects = os.listdir(user_settings.archdaily_projects_dir)
    ctx.set_total(len(_all_projects))

    if _all_projects == 0:
        raise Exception("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®")

    ctx.report_msg("æ­£åœ¨æ‰«ææœ¬åœ°æ–‡ä»¶...")
    g.project_id_queue = []
    num_projects_with_no_content_html = 0
    for project_id in _all_projects:
        ctx.update(1)
        if ctx.should_stop:
            break
        if skip_exist:
            json_file_path = os.path.join(user_settings.archdaily_projects_dir, project_id, 'content.json')
            if os.path.exists(json_file_path):
                continue
        ctx.report_project_start(project_id)
        html_file_path = os.path.join(user_settings.archdaily_projects_dir, project_id, 'content.html')

        if os.path.exists(html_file_path):
            g.project_id_queue.append(project_id)
        else:
            num_projects_with_no_content_html += 1
        ctx.report_project_success(project_id)
    ctx.custom_data['num_projects_with_no_content_html'] = num_projects_with_no_content_html
    ctx.custom_data['final_msg'] = f"å…±è®¡{len(_all_projects)}ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­{len(g.project_id_queue)}ä¸ªé¡¹ç›®å·²æ·»åŠ åˆ°é˜Ÿåˆ—"


def archdaily__scan_projects_folder_for_downloading_images(ctx: WorkingContext, *args):
    return common__scan_projects_folder_for_downloading_images(ctx, user_settings.archdaily_projects_dir, *args)


def archdaily__download_projects_html_to_local(ctx: WorkingContext, *args):
    from utils.html_utils import request_project_html_archdaily, flush_success_queue
    from concurrent.futures import ThreadPoolExecutor, as_completed

    # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½invalid_project_ids
    if os.path.exists(user_settings.archdaily_invalid_projects_ids_path):
        with open(user_settings.archdaily_invalid_projects_ids_path, 'r', encoding='utf-8') as f:
            invalid_project_ids = set(json.load(f))
    else:
        invalid_project_ids = set()
    _total = len(g.project_id_queue)
    assert _total > 0, "æ²¡æœ‰é¡¹ç›®éœ€è¦ä¸‹è½½"
    ctx.set_total(_total)
    saving_gap = max(_total // 20, 100)  # æ€»æ•°å¹³å‡åˆ†20ä»½ï¼Œä½†æ˜¯æœ€å°æ¯100è½®ä¿å­˜ä¸€æ¬¡

    def save_invalid_project_ids():
        with open(user_settings.archdaily_invalid_projects_ids_path, 'w', encoding='utf-8') as f:
            logging.info("ä¿å­˜invalid_project_ids")
            json.dump(list(invalid_project_ids), f, ensure_ascii=False, indent=4)

    def _get_html_content(project_id: str, i: int):
        if ctx.should_stop:
            return
        ctx.report_project_start(project_id)
        success = request_project_html_archdaily(project_id, i, _total, invalid_project_ids,
                                                 force_update=False)
        if success is True:
            ctx.report_project_success(project_id)
        elif success is False:
            ctx.report_project_failed(project_id)
        else:
            ctx.report_project_complete(project_id)
        ctx.update(1)
        if i % saving_gap == 0:
            save_invalid_project_ids()

    with ThreadPoolExecutor(max_workers=32) as executor:
        futures = (executor.submit(_get_html_content, project_id, i) for i, project_id in
                   enumerate(g.project_id_queue))
        for future in as_completed(futures):
            future.result()

    flush_success_queue('content_html')
    save_invalid_project_ids()


def archdaily__parse_htmls(ctx: WorkingContext, flags_state, *args):
    from utils.html_utils import parse_project_content_archdaily, flush_success_queue, ArchdailyFlags
    from concurrent.futures import ThreadPoolExecutor, as_completed

    _total = len(g.project_id_queue)
    assert _total > 0, "æ²¡æœ‰é¡¹ç›®éœ€è¦è§£æ"

    ctx.set_total(_total)

    # get flags
    combined_flags = ArchdailyFlags.NONE
    for flag_name, value in flags_state.items():
        if value:
            logging.info(f"{flag_name} is On")
            flag = g.flag_name_to_flag['archdaily'][flag_name]
            combined_flags |= flag

    def _parse_project_content(project_id: str, i: int):
        if ctx.should_stop:
            return
        time.sleep(0.02)

        ctx.report_project_start(project_id)

        changed = parse_project_content_archdaily(project_id, i, _total, flags=combined_flags)
        if changed is True:
            ctx.report_project_success(project_id)
        elif changed is False:
            ctx.report_project_failed(project_id)
        else:
            ctx.report_project_complete(project_id)

        ctx.update(1)

    with ThreadPoolExecutor(max_workers=64) as executor:
        # æ³¨æ„æ‹¬å·ï¼Œä½¿ç”¨Generatorè€ŒéList
        futures = (executor.submit(_parse_project_content, project_id, i) for i, project_id in
                   enumerate(g.project_id_queue))
        logging.info("å¼€å§‹è§£æé¡µé¢å†…å®¹... å¦‚æœé‡åˆ°image_galleryä¸ºç©ºçš„æƒ…å†µï¼Œå¯èƒ½éœ€è¦ç­‰å¾…è¿”å›image_galleryç»“æœ")
        for future in tqdm(as_completed(futures), total=len(g.project_id_queue)):
            future.result()

    flush_success_queue('content_json')


def archdaily__download_gallery_images(ctx: WorkingContext, *args):
    return common__download_gallery_images(ctx, user_settings.archdaily_projects_dir, *args)


def archdaily__upload_content(ctx: WorkingContext, skip_exist: bool = True, *args):
    return common__upload_content(ctx, user_settings.mongodb_archdaily_db_name, user_settings.archdaily_projects_dir, skip_exist, *args)


def archdaily__scan_embedding_db(ctx: WorkingContext, skip_exist: bool = True, *args):
    return common__scan_embedding_db(ctx, user_settings.mongodb_archdaily_db_name, user_settings.archdaily_projects_dir, skip_exist, *args)


def archdaily__calculate_text_embedding_using_multimodal_embedding_v1_api(ctx: WorkingContext,
                                                                          chunk_size=500,
                                                                          chunk_overlap=50,
                                                                          *args):
    return common__calculate_text_embedding_using_multimodal_embedding_v1_api(ctx, user_settings.mongodb_archdaily_db_name,
                                                                              chunk_size, chunk_overlap, *args)


def archdaily__calculate_text_embedding_using_gme_Qwen2_VL_2B_api(ctx: WorkingContext,
                                                                  chunk_size=500,
                                                                  chunk_overlap=50,
                                                                  *args):
    return common__calculate_text_embedding_using_gme_Qwen2_VL_2B_api(ctx, user_settings.mongodb_archdaily_db_name,
                                                                      chunk_size, chunk_overlap, *args)


# endregion


# region gooood
def gooood__scrap_pages(ctx: WorkingContext, scrap_all=False, start_page=1, end_page=1, skip_exist=False, *args):
    if start_page > end_page:
        start_page, end_page = end_page, start_page
    if scrap_all:
        start_page = 1
    if not scrap_all:
        ctx.set_total(end_page - start_page + 1)
    else:
        ctx.set_total(1)
    page = start_page - 1
    page_count = 0
    pages_folder = os.path.join(user_settings.gooood_results_dir, "pages")
    os.makedirs(pages_folder, exist_ok=True)
    while True:
        page += 1

        if ctx.should_stop:
            break
        if not scrap_all and page > end_page:
            ctx.report_msg("å·²åˆ°è¾¾æŒ‡å®šé¡µæ•°")
            logging.info(f"å·²åˆ°è¾¾æŒ‡å®šé¡µæ•°")
            break
        page_count += 1
        ctx.update(1)
        if scrap_all:
            ctx.set_total(page_count + 1)
        json_path = os.path.join(pages_folder, f"page_{str(page).zfill(5)}.json")
        if skip_exist and os.path.exists(json_path):
            continue

        ctx.report_project_start(page)
        try:
            # Send a GET request to the website
            url = user_settings.gooood_base_url.replace("<page>", str(page))
            response = requests.get(url, headers=user_settings.headers)
            if response.status_code != 200:
                ctx.report_project_failed(page)
                continue
            data = json.loads(response.text)
            if len(data) == 0:
                logging.info(f"å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                ctx.report_project_complete(page)
                break
            with open(json_path, "w", encoding="utf-8") as f:
                f.write(json.dumps(data, ensure_ascii=False, indent=4))
            ctx.report_project_success(page)
        except requests.exceptions.RequestException as e:
            print(f"Error accessing the website: {e}")
            ctx.report_project_failed(page)
        except Exception as e:
            print(f"An error occurred: {e}")
            ctx.report_project_failed(page)


def gooood__init_projects(ctx: WorkingContext, skip_exist=True, *args):
    pages_folder = os.path.join(user_settings.gooood_results_dir, "pages")
    if not os.path.exists(pages_folder):
        raise Exception("pagesæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
    all_pages = os.listdir(pages_folder)
    if not all_pages:
        raise Exception("pagesæ–‡ä»¶å¤¹ä¸ºç©º")
    ctx.set_total(len(all_pages))
    for page in tqdm(all_pages):
        ctx.update(1)
        page_path = os.path.join(pages_folder, page)
        with open(page_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        for project_data in data:
            project_id = str(project_data['id'])
            project_folder = os.path.join(user_settings.gooood_projects_dir, project_id)
            project_data_path = os.path.join(project_folder, f"{project_id}.json")
            if skip_exist and os.path.exists(project_data_path):
                continue
            os.makedirs(project_folder, exist_ok=True)
            with open(project_data_path, "w", encoding="utf-8") as f:
                f.write(json.dumps(project_data, ensure_ascii=False, indent=4))


def gooood__parse_projects(ctx: WorkingContext, flags_state, skip_exist=False, *args):
    from utils.html_utils import parse_project_content_gooood, flush_success_queue, GoooodFlags
    from concurrent.futures import ThreadPoolExecutor, as_completed

    # get flags
    combined_flags = GoooodFlags.NONE
    for flag_name, value in flags_state.items():
        if value:
            logging.info(f"{flag_name} is On")
            flag = g.flag_name_to_flag['gooood'][flag_name]
            combined_flags |= flag
    if not os.path.exists(user_settings.gooood_projects_dir):
        raise Exception("projectsæ–‡ä»¶å¤¹ä¸å­˜åœ¨")

    all_projects = os.listdir(user_settings.gooood_projects_dir)
    if not all_projects:
        raise Exception("projectsæ–‡ä»¶å¤¹ä¸ºç©º")
    _total = len(all_projects)
    ctx.set_total(_total)

    def _parse_project_content(project_id: str, i: int):
        if ctx.should_stop:
            return
        time.sleep(0.1)
        ctx.update(1)
        project_data_path = os.path.join(user_settings.gooood_projects_dir, project_id, f"{project_id}.json")
        project_content_path = os.path.join(user_settings.gooood_projects_dir, project_id, f"content.json")
        if skip_exist and os.path.exists(project_content_path):
            return
        ctx.report_project_start(project_id)
        changed = parse_project_content_gooood(project_id, i, _total, flags=combined_flags)
        if changed is True:
            ctx.report_project_success(project_id)
        elif changed is False:
            ctx.report_project_failed(project_id)
        else:
            ctx.report_project_complete(project_id)

    with ThreadPoolExecutor(max_workers=64) as executor:
        futures = (executor.submit(_parse_project_content, project_id, i) for i, project_id in
                   enumerate(all_projects))
        for future in tqdm(as_completed(futures), total=len(all_projects)):
            future.result()

    flush_success_queue('content_json')


def gooood__scan_projects_folder_for_downloading_images(ctx: WorkingContext, *args):
    return common__scan_projects_folder_for_downloading_images(ctx, user_settings.gooood_projects_dir, *args)


def gooood__download_gallery_images(ctx: WorkingContext, *args):
    return common__download_gallery_images(ctx, user_settings.gooood_projects_dir, *args)



def gooood__upload_content(ctx: WorkingContext, skip_exist: bool = True, *args):
    return common__upload_content(ctx, user_settings.mongodb_gooood_db_name, user_settings.gooood_projects_dir, skip_exist, *args)


def gooood__scan_embedding_db(ctx: WorkingContext, skip_exist: bool = True, *args):
    return common__scan_embedding_db(ctx, user_settings.mongodb_gooood_db_name, user_settings.gooood_projects_dir, skip_exist, *args)


def gooood__calculate_text_embedding_using_multimodal_embedding_v1_api(ctx: WorkingContext,
                                                                          chunk_size=500,
                                                                          chunk_overlap=50,
                                                                          *args):
    return common__calculate_text_embedding_using_multimodal_embedding_v1_api(ctx, user_settings.mongodb_gooood_db_name,
                                                                              chunk_size, chunk_overlap, *args)


def gooood__calculate_text_embedding_using_gme_Qwen2_VL_2B_api(ctx: WorkingContext,
                                                                  chunk_size=500,
                                                                  chunk_overlap=50,
                                                                  *args):
    return common__calculate_text_embedding_using_gme_Qwen2_VL_2B_api(ctx, user_settings.mongodb_gooood_db_name,
                                                                      chunk_size, chunk_overlap, *args)
# endregion


# region common
def common__scan_projects_folder_for_downloading_images(ctx: WorkingContext, projects_dir, *args):
    _all_projects = os.listdir(projects_dir)
    if _all_projects == 0:
        raise Exception("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®")

    ctx.set_total(len(_all_projects))

    content_not_exist_count = 0
    g.project_id_queue = []

    # éå†é¡¹ç›®ç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
    for folder_name in tqdm(_all_projects):
        if ctx.should_stop:
            break
        ctx.update(1)
        folder_path = os.path.join(projects_dir, folder_name)
        if not os.path.isdir(folder_path):
            continue
        json_file_path = os.path.join(folder_path, 'content.json')
        if not os.path.exists(json_file_path):
            content_not_exist_count += 1  # content.json does not exist, add to content_not_exist_count
            continue
        image_gallery_folder = os.path.join(folder_path, 'image_gallery', 'large')
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        image_gallery_images = data.get('image_gallery', [])
        if not image_gallery_images:
            continue
        if not os.path.exists(image_gallery_folder):
            g.project_id_queue.append(folder_name)
            continue
        image_gallery_names = os.listdir(image_gallery_folder)
        if len(image_gallery_names) < len(image_gallery_images):
            g.project_id_queue.append(folder_name)

    ctx.custom_data[
        'final_msg'] = f"å·²æ‰«æ{len(_all_projects)}ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­{content_not_exist_count}ä¸ªé¡¹ç›®æ²¡æœ‰content.jsonæ–‡ä»¶ï¼Œ{len(g.project_id_queue)}ä¸ªé¡¹ç›®éœ€è¦ä¸‹è½½å›¾åƒ"


def common__download_gallery_images(ctx: WorkingContext, projects_dir, *args):
    from utils.html_utils import download_images_common
    from concurrent.futures import ThreadPoolExecutor, as_completed
    _total = len(g.project_id_queue)
    assert _total > 0, "æ²¡æœ‰éœ€è¦ä¸‹è½½å›¾åƒçš„é¡¹ç›®"
    ctx.set_total(_total)

    def _on_img_index_change(project_id, img_index, img_total):
        ctx.report_project_sub_curr(project_id, img_index)
        ctx.report_project_sub_total(project_id, img_total)

    def _download_images(project_id: str, i: int):
        if ctx.should_stop:
            return

        ctx.report_project_start(project_id)
        success = download_images_common(projects_dir, project_id, i, _total, 'large',
                                         img_index_change_callback=_on_img_index_change)
        if success is True:
            ctx.report_project_success(project_id)
        elif success is False:
            ctx.report_project_failed(project_id)
        else:
            ctx.report_project_complete(project_id)
        ctx.update(1)

    with ThreadPoolExecutor(max_workers=48) as executor:
        futures = (executor.submit(_download_images, project_id, i) for i, project_id in enumerate(g.project_id_queue))
        for future in as_completed(futures):
            future.result()

    logging.info('complete')


def common__upload_content(ctx: WorkingContext, db_name, projects_dir, skip_exist: bool = True, *args):
    if g.mongo_client is None:
        raise Exception("MongoDBè¿æ¥å¤±è´¥")
    db = g.mongo_client[db_name]

    content_collection = db['content_collection']

    all_projects = os.listdir(projects_dir)
    ctx.set_total(len(all_projects))

    def _handle_project(project_id: str):
        if ctx.should_stop:
            return
        ctx.report_project_start(project_id)
        ctx.update(1)
        project_path = os.path.join(projects_dir, project_id)

        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥ _id
        if skip_exist:
            existing_doc = content_collection.find_one({'_id': project_id})
            if existing_doc:
                # logging.info(f"project: {project_id} å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œè·³è¿‡å¤„ç†")
                ctx.report_project_complete(project_id)
                return

        # è¯»å–content.json
        content_json_path = os.path.join(project_path, 'content.json')
        if not os.path.exists(content_json_path):
            logging.warning(f"project: {project_id} content.jsonæ–‡ä»¶ä¸å­˜åœ¨")
            ctx.report_project_failed(project_id)
            return

        try:
            with open(content_json_path, 'r', encoding='utf-8') as f:
                content_data = json.load(f)
        except Exception as e:
            logging.error(f"project: {project_id} content.jsonæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
            os.remove(content_json_path)
            ctx.report_project_failed(project_id)
            return
        # æ’å…¥æˆ–æ›´æ–°contentæ•°æ®
        content_doc = {'_id': project_id}
        content_doc.update(content_data)
        content_result = content_collection.update_one(
            {'_id': project_id},
            {'$set': content_doc},
            upsert=True  # ä¿®æ”¹ä¸º upsert=Trueï¼Œç¡®ä¿ä¸å­˜åœ¨æ—¶æ’å…¥
        )
        # åŒºåˆ†æ’å…¥å’Œæ›´æ–°æ“ä½œ
        # if content_result.upserted_id:
        #     logging.info(f"project: {project_id} æ’å…¥æˆåŠŸ")
        # else:
        #     logging.info(f"project: {project_id} æ›´æ–°æˆåŠŸï¼Œä¿®æ”¹è®¡æ•°: {content_result.modified_count}")
        ctx.report_project_success(project_id)

    with ThreadPoolExecutor(max_workers=16) as executor:
        futures = (executor.submit(_handle_project, project_id) for project_id in all_projects)
        for future in tqdm(as_completed(futures), total=len(all_projects)):
            future.result()
    logging.info('complete')


def common__scan_embedding_db(ctx: WorkingContext, db_name, projects_dir, skip_exist: bool = True, *args):
    if g.mongo_client is None:
        raise Exception("MongoDBè¿æ¥å¤±è´¥")
    db = g.mongo_client[db_name]
    content_collection = db['content_collection']
    content_embedding_collection = db['content_embedding']

    # éå†æ¯ä¸ªé¡¹ç›®
    all_projects = os.listdir(projects_dir)
    ctx.set_total(len(all_projects))
    g.project_id_queue.clear()
    for project_id in tqdm(all_projects):
        if ctx.should_stop:
            break
        ctx.update(1)
        ctx.report_project_start(project_id)
        # åˆ¤æ–­å½“å‰ project_id æ˜¯å¦å·²å­˜åœ¨äº content_embedding_collection ä¸­
        existing = content_embedding_collection.find_one(
            {"project_id": project_id},
            {"_id": 1},  # åªè¿”å›_idå­—æ®µ
            limit=1
        )
        existing_embeddings = existing is not None
        # æ ¹æ®ç”¨æˆ·é€‰é¡¹å†³å®šæ˜¯å¦è·³è¿‡æˆ–è¦†ç›–
        if existing_embeddings:
            # å¦‚æœembeddingæ•°æ®åº“ä¸­å­˜åœ¨å½“å‰é¡¹ç›®ï¼Œåˆ™æ ¹æ®ç”¨æˆ·é€‰é¡¹å†³å®šæ˜¯å¦è·³è¿‡æˆ–è¦†ç›–
            if skip_exist:
                # logging.info(f"project: {project_id} å·²å­˜åœ¨äº content_embedding_collection ä¸­ï¼Œè·³è¿‡å¤„ç†")
                ctx.report_project_complete(project_id)
                continue
            else:
                # åˆ é™¤æ‰€æœ‰ä¸å½“å‰ project_id ç›¸å…³çš„æ–‡æ¡£
                content_embedding_collection.delete_many({'project_id': project_id})
                logging.info(f"project: {project_id} å·²å­˜åœ¨äº content_embedding_collection ä¸­ï¼Œåˆ é™¤ç°æœ‰æ•°æ®å¹¶é‡æ–°å¤„ç†")
                g.project_id_queue.append(project_id)
                ctx.report_project_success(project_id)
                continue
        # å¦‚æœé¡¹ç›®åœ¨embeddingæ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼Œåˆ™ç»§ç»­å¤„ç†
        # ä»content_collectionä¸­æå–main_content
        content_doc = content_collection.find_one({'_id': project_id})
        if not content_doc:
            # é¡¹ç›®åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†
            ctx.report_project_complete(project_id)
            continue
        if 'main_content' not in content_doc:
            # å¦‚æœé¡¹ç›®åœ¨æ•°æ®åº“ä¸­æ²¡æœ‰main_contentå­—æ®µï¼Œåˆ™è·³è¿‡å¤„ç†å¹¶è­¦å‘Š
            logging.warning(f"project: {project_id} æ²¡æœ‰main_contentå­—æ®µ")
            ctx.report_project_failed(project_id)
            continue
        # å¦‚æœembeddingä¸­ä¸å­˜åœ¨project å¹¶ä¸” content_collectionä¸­å­˜åœ¨ï¼Œåˆ™æ­£å¸¸æ·»åŠ åˆ°é˜Ÿåˆ—
        g.project_id_queue.append(project_id)
        ctx.report_project_success(project_id)


def common__calculate_text_embedding_using_multimodal_embedding_v1_api(ctx: WorkingContext, db_name,
                                                                       chunk_size=500,
                                                                       chunk_overlap=50,
                                                                       *args):
    warnings.warn(
        "calculate_text_embedding_using_multimodal_embedding_v1_api å·²å¼ƒç”¨",
        DeprecationWarning,
        stacklevel=2
    )
    _total = len(g.project_id_queue)
    assert _total > 0, "æ²¡æœ‰éœ€è¦å¤„ç†çš„é¡¹ç›®"
    ctx.set_total(_total)

    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from apis.multimodal_embedding_v1_api import embed_text
    if g.mongo_client is None:
        raise Exception("MongoDBè¿æ¥å¤±è´¥")
    db = g.mongo_client[db_name]
    content_collection = db['content_collection']
    content_embedding_collection = db['content_embedding']

    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,  # æ¯æ®µæœ€å¤§é•¿åº¦
        chunk_overlap=chunk_overlap  # æ®µä¸æ®µä¹‹é—´çš„é‡å é•¿åº¦
    )
    valid_api_keys = user_settings.api_keys.copy()

    def _handle_project(project_id: str, i: int):
        if ctx.should_stop:
            return
        ctx.update(1)
        ctx.report_project_start(project_id)

        content_doc = content_collection.find_one({'_id': project_id})
        main_content = content_doc['main_content']

        text_contents = [item['content'] for item in main_content if item['type'] == 'text']
        chunks: list[dict[str: any]] = []
        for text_idx, text in enumerate(text_contents):
            chunks.extend([{'text_idx': text_idx, 'chunk_idx': chunk_idx, 'content': chunk} for chunk_idx, chunk in
                           enumerate(text_splitter.split_text(text))])

        api_key = valid_api_keys.pop(0)  # æ‹¿å–api key
        query_chunks = chunks.copy()
        ctx.report_project_sub_total(project_id, len(chunks))
        sub_curr = 0
        for attempt in range(5):  # æœ€å¤šå°è¯•5æ¬¡
            curr_chunks = query_chunks.copy()
            query_chunks = []
            for chunk_data in curr_chunks:

                text_idx = chunk_data['text_idx']
                chunk_idx = chunk_data['chunk_idx']
                content = chunk_data['content']
                # è·å–åµŒå…¥å‘é‡
                embedding_vector, status_code = embed_text(content, api_key=api_key)
                if embedding_vector is None:
                    query_chunks.append(chunk_data)
                else:
                    # æ’å…¥åˆ°content_embeddingé›†åˆ
                    embedding_doc = {
                        'project_id': project_id,
                        'embedding': embedding_vector,
                        'text_content': content,
                        'text_idx': text_idx,
                        'chunk_idx': chunk_idx
                    }
                    result = content_embedding_collection.insert_one(embedding_doc)
                    sub_curr += 1
                    ctx.report_project_sub_curr(project_id, sub_curr)
            if not query_chunks:
                break  # å¦‚æœæ²¡æœ‰å¤±è´¥çš„chunkï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            logging.info(f"project: {project_id} ç¬¬{attempt + 1}/5æ¬¡å°è¯•ï¼Œå‰©ä½™å¤±è´¥chunkæ•°: {len(query_chunks)}")
        valid_api_keys.append(api_key)  # è¿”è¿˜apikey

        if not query_chunks:
            ctx.report_project_success(project_id)
        else:
            ctx.report_project_failed(project_id)
            logging.warning(f"project: {project_id} æœ‰{len(query_chunks)}ä¸ªchunkæœªèƒ½æˆåŠŸè·å–åµŒå…¥å‘é‡")

    with ThreadPoolExecutor(max_workers=len(user_settings.api_keys)) as executor:
        futures = (executor.submit(_handle_project, project_id, i) for i, project_id in enumerate(g.project_id_queue))
        for future in as_completed(futures):
            future.result()


def common__calculate_text_embedding_using_gme_Qwen2_VL_2B_api(ctx: WorkingContext, db_name,
                                                               chunk_size=500,
                                                               chunk_overlap=50,
                                                               *args):
    _total = len(g.project_id_queue)
    assert _total > 0, "æ²¡æœ‰é¡¹ç›®éœ€è¦ä¸‹è½½"
    ctx.set_total(_total)

    if g.mongo_client is None:
        raise Exception("MongoDBè¿æ¥å¤±è´¥")
    db = g.mongo_client[db_name]
    content_collection = db['content_collection']
    content_embedding_collection = db['content_embedding']

    ctx.report_msg("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from apis.gme_Qwen2_vl_2B_api import get_text_embeddings

    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,  # æ¯æ®µæœ€å¤§é•¿åº¦
        chunk_overlap=chunk_overlap  # æ®µä¸æ®µä¹‹é—´çš„é‡å é•¿åº¦
    )
    project_id_queue = deque(g.project_id_queue)
    _doc_buffer_queue = deque()
    _embedding_complete = False

    def _embedding_thread():
        while len(project_id_queue) > 0:
            if ctx.should_stop:
                project_id_queue.clear()
                break
            if len(_doc_buffer_queue) > 100:  # put up to 50 projects in queue
                time.sleep(0.2)
                continue
            project_id = project_id_queue.popleft()
            ctx.update(1)
            ctx.report_project_start(project_id)
            # æ­¤å‰scanæ—¶å·²ç»ç¡®ä¿éƒ½æ˜¯å­˜åœ¨idå’Œmaincontentçš„ï¼Œå› æ­¤æ­¤å¤„å¯ä»¥ç›´æ¥å–ç”¨
            content_doc = content_collection.find_one({'_id': project_id})
            main_content = content_doc['main_content']
            text_contents = [item['content'] for item in main_content if item['type'] == 'text']
            chunks: list[dict[str: any]] = []
            for text_idx, text in enumerate(text_contents):
                chunks.extend([{'text_idx': text_idx, 'chunk_idx': chunk_idx, 'content': chunk} for chunk_idx, chunk in
                               enumerate(text_splitter.split_text(text))])
            chunks = [chunk for chunk in chunks if chunk['content'].strip() != '']
            if len(chunks) == 0:
                ctx.report_project_failed(project_id)
                logging.warning(f"project: {project_id} æ²¡æœ‰æ–‡æœ¬å†…å®¹")
                continue
            ctx.report_project_sub_total(project_id, len(chunks))
            input_texts = [chunk['content'] for chunk in chunks]

            ctx.report_project_sub_curr(project_id, "EBD")
            embedding_vectors = get_text_embeddings(input_texts, batch_size=min(len(input_texts), 32),
                                                    show_progress_bar=False)
            # åˆ¤æ–­æ˜¯å¦æœ‰NaN
            if np.isnan(embedding_vectors).any():
                ctx.report_project_failed(project_id)
                logging.error(f"project: {project_id} æœ‰NaNå€¼ embedding_vectors")
                continue
            buffer = []
            for i, chunk_data in enumerate(chunks):
                text_idx = chunk_data['text_idx']
                chunk_idx = chunk_data['chunk_idx']
                content = chunk_data['content']
                embedding_vector = embedding_vectors[i].tolist()
                embedding_doc = {
                    'project_id': project_id,
                    'chunk_id': f'{project_id}-{text_idx}-{chunk_idx}',
                    'embedding': embedding_vector,
                    'text_content': content,
                    'text_idx': text_idx,
                    'chunk_idx': chunk_idx,
                }
                buffer.append(embedding_doc)
            if len(buffer) == 0:
                ctx.report_project_failed(project_id)
                logging.warning(f"project: {project_id} æ²¡æœ‰ä»»ä½•æ•°æ®")
                continue
            _doc_buffer_queue.append(buffer)
            ctx.report_project_sub_curr(project_id, "InQ")
    def _upload_doc_thread():
        time.sleep(random.random())
        while True:
            if len(_doc_buffer_queue) == 0:
                if len(project_id_queue) > 0:
                    time.sleep(0.2)  # ç­‰å¾…project_id_queueé˜Ÿåˆ—ä¸ºç©ºï¼Œå†é€€å‡ºå¾ªç¯ï¼Œ å¦åˆ™ä¸€ç›´å¾…å‘½
                    continue
                else:
                    break
            try:
                buffer = _doc_buffer_queue.popleft()
            except Exception as e:
                logging.warning(f"_upload_doc_thread error: {e}, this is not supposed to happen")
                continue
            project_id = buffer[0]['project_id']
            ctx.report_project_sub_curr(project_id, f"WDB")
            result = content_embedding_collection.insert_many(buffer)
            ctx.report_project_success(project_id)


    embedding_thread = threading.Thread(target=_embedding_thread)
    embedding_thread.start()
    upload_thread = threading.Thread(target=_upload_doc_thread)
    upload_thread.start()
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    embedding_thread.join()
    upload_thread.join()

    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def common__fix_nan_embeddings_using_gme_Qwen2_VL_2B_api(ctx: WorkingContext, db_name):
    if g.mongo_client is None:
        raise Exception("MongoDBè¿æ¥å¤±è´¥")
    db = g.mongo_client[db_name]
    content_embedding_collection = db['content_embedding']
    logging.info("counting documents")
    document_count = content_embedding_collection.count_documents({})
    logging.info(f"document count = {document_count}")
    cursor = content_embedding_collection.find({})
    from apis.gme_Qwen2_vl_2B_api import get_text_embeddings
    modified_count = 0
    ctx.set_total(document_count)
    for doc in tqdm(cursor, total=document_count):
        if ctx.should_stop:
            break
        ctx.update(1)
        assert 'project_id' in doc, "é‡åˆ°ä¸¥é‡é”™è¯¯ï¼šæ–‡æ¡£ç¼ºå°‘project_idå­—æ®µ"
        if "embedding" not in doc:
            logging.error(f"one doc of project: {doc['project_id']} has not embedding")
            continue
        if 'text_content' not in doc:
            logging.error(f"one doc of project: {doc['project_id']} has not text_content")
            continue
        embedding = doc["embedding"]
        # æ£€æŸ¥embeddingæ˜¯å¦ä¸ºåˆ—è¡¨
        if not isinstance(embedding, list):
            logging.error(f"find one document:{doc['project_id']} has no list embedding")
            continue

        # æ–°å¢ï¼šæ£€æŸ¥embeddingä¸­æ˜¯å¦å­˜åœ¨NaNå€¼
        if np.isnan(np.array(embedding)).any():
            ctx.report_project_start(doc['project_id'])
            logging.info(f"find one document:{doc['project_id']} with NaN")
            text_content = doc['text_content']
            embedding_vectors = get_text_embeddings(text_content, batch_size=1,
                                                    show_progress_bar=False)
            new_embedding = embedding_vectors[0].tolist()
            doc.update({"embedding": new_embedding})
            content_embedding_collection.update_one(
                {"_id": doc["_id"]},
                {"$set": {"embedding": new_embedding}}
            )
            modified_count += 1
            ctx.report_project_success(doc['project_id'])

# endregion
