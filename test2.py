import os
import json
import time
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

# ArchDaily æœç´¢ç»“æœ URL
ARCHDAILY_SEARCH_URL = "https://www.archdaily.com/search/projects?adkq=building&page="

# å›¾ç‰‡ä¿å­˜ç›®å½•
IMG_FOLDER = "archdaily_images"
if not os.path.exists(IMG_FOLDER):
    os.makedirs(IMG_FOLDER)

# é¡¹ç›®æ•°æ®å­˜å‚¨æ–‡ä»¶
DATA_FILE = "projects.json"

def get_project_links(page=1):
    """ ä½¿ç”¨ Selenium çˆ¬å–æœç´¢ç»“æœé¡µé¢ """
    url = ARCHDAILY_SEARCH_URL + str(page)
    print(f"ğŸ” çˆ¬å–æœç´¢é¡µ: {url}")

    # å¯åŠ¨æµè§ˆå™¨
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # æ— å¤´æ¨¡å¼
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    driver.get(url)
    driver.implicitly_wait(5)

    # æ‰¾åˆ°é¡¹ç›®é“¾æ¥
    project_elements = driver.find_elements(By.CSS_SELECTOR, "div.project-card a")
    project_links = ["https://www.archdaily.com" + element.get_attribute("href") for element in project_elements]

    driver.quit()

    if not project_links:
        print("âš ï¸ æœªæ‰¾åˆ°é¡¹ç›®å…ƒç´ ï¼Œå¯èƒ½æ˜¯ç½‘é¡µç»“æ„æ”¹å˜æˆ–è¢«åçˆ¬ï¼")
        return []

    print(f"âœ… æ‰¾åˆ° {len(project_links)} ä¸ªé¡¹ç›®")
    return project_links[:5]  # ä»…è·å–å‰ 5 ä¸ª

def scrape_project_details(project_url):
    """ è·å–é¡¹ç›®è¯¦æƒ…ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€ä»‹ç»å’Œå›¾ç‰‡ """
    print(f"ğŸ“¡ çˆ¬å–é¡¹ç›®é¡µé¢: {project_url}")

    response = requests.get(project_url)
    if response.status_code != 200:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {project_url}ï¼ŒçŠ¶æ€ç : {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # è·å–é¡¹ç›®æ ‡é¢˜
    title_tag = soup.find("h1", class_="afd-title")
    title = title_tag.text.strip() if title_tag else "æœªçŸ¥é¡¹ç›®"

    # è·å–é¡¹ç›®ä»‹ç»
    description_tag = soup.find("div", class_="afd-paragraph")
    description = description_tag.text.strip() if description_tag else "æš‚æ— ä»‹ç»"

    # è·å–æ‰€æœ‰å›¾ç‰‡
    img_tags = soup.find_all("img", class_="picture--content__img")
    img_urls = [img["src"] for img in img_tags if "src" in img.attrs]

    # ä¸‹è½½å›¾ç‰‡
    saved_images = []
    for img_url in img_urls[:3]:  # ä»…ä¸‹è½½å‰ 3 å¼ å›¾ç‰‡ï¼Œé¿å…è¿‡å¤š
        img_path = download_image(img_url, title)
        if img_path:
            saved_images.append(img_path)

    return {
        "title": title,
        "description": description,
        "images": saved_images,
        "url": project_url
    }

def download_image(img_url, title):
    """ ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜ """
    print(f"ğŸŒ ä¸‹è½½å›¾ç‰‡: {img_url}")

    try:
        response = requests.get(img_url, stream=True)
        if response.status_code == 200:
            filename = f"{IMG_FOLDER}/{title.replace(' ', '_')}.jpg"
            with open(filename, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)
            print(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {filename}")
            return filename
        else:
            print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url}")
    except Exception as e:
        print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å‡ºé”™: {e}")
    return None

def save_data(data):
    """ ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶ """
    with open(DATA_FILE, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=4)
    print(f"ğŸ’¾ æ•°æ®ä¿å­˜æˆåŠŸ: {DATA_FILE}")

def main():
    """ çˆ¬å– ArchDaily é¡¹ç›®æ•°æ® """
    print("ğŸš€ å¼€å§‹çˆ¬å– ArchDaily é¡¹ç›®æ•°æ®...")

    project_links = get_project_links()
    if not project_links:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®ï¼Œç¨‹åºé€€å‡º")
        return

    all_projects = []
    for project_url in project_links:
        project_data = scrape_project_details(project_url)
        if project_data:
            all_projects.append(project_data)

        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«è¢«å°

    # ä¿å­˜æ•°æ®
    save_data(all_projects)
    print("ğŸ‰ çˆ¬å–å®Œæˆï¼")

if __name__ == "__main__":
    main()